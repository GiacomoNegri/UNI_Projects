{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bfc0f0-c7e4-4eb1-b36c-9d3a63807a72",
   "metadata": {},
   "source": [
    "# DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65576ac3-cd72-428d-b331-cf6ebb147187",
   "metadata": {},
   "source": [
    "## Track I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34fea4a3-ce21-400b-b85c-53b2b39789bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (w_gram: (1, 2), max_df=0.8, min_df=1): 0.08962108211013144\n",
      "Best Params: ((1, 2), 0.8, 1) with BLEU Score: 0.08962108211013144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "def load_data():\n",
    "    dev_responses = pd.read_csv('dev_responses.csv')\n",
    "    train_responses = pd.read_csv('train_responses.csv')\n",
    "    return pd.concat([dev_responses, train_responses], ignore_index=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    # text = re.sub(r'\\b\\d+\\b', '', text)  # Remove isolated numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "original_data = load_data()\n",
    "original_data['user_prompt'] = original_data['user_prompt'].astype(str)\n",
    "original_data['model_response'] = original_data['model_response'].astype(str)\n",
    "original_data['processed_prompt'] = original_data['user_prompt'].apply(preprocess_text)\n",
    "\n",
    "X_train, X_test = train_test_split(original_data, test_size=0.2, random_state=42)\n",
    "\n",
    "def compute_tfidf(X_train, X_test, ngram_range_word,ngram_range_char, max_df, min_df):\n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=ngram_range_word, max_df=max_df, min_df=min_df, analyzer='word', sublinear_tf=True)\n",
    "    char_vectorizer = TfidfVectorizer(ngram_range=ngram_range_char, max_df=max_df, min_df=min_df, analyzer='char', sublinear_tf=True)\n",
    "    \n",
    "    vectorizer = FeatureUnion([(\"word_tfidf\", word_vectorizer), (\"char_tfidf\", char_vectorizer)])\n",
    "    \n",
    "    tfidf_train = vectorizer.fit_transform(X_train['processed_prompt'])\n",
    "    tfidf_test = vectorizer.transform(X_test['processed_prompt'])\n",
    "    \n",
    "    return vectorizer, tfidf_train, tfidf_test\n",
    "\n",
    "def find_best_responses(tfidf_train, tfidf_test, X_train):\n",
    "    similarities = cosine_similarity(tfidf_test, tfidf_train)\n",
    "    # similarities=-euclidean_distances(tfidf_test, tfidf_train)\n",
    "    best_indices = np.argmax(similarities, axis=1)\n",
    "    \n",
    "    retrieved_responses = X_train.iloc[best_indices]['model_response'].values\n",
    "    return retrieved_responses\n",
    "\n",
    "smoothing_function = SmoothingFunction().method3\n",
    "\n",
    "def compute_bleu(ref_text, hyp_text):\n",
    "    return sentence_bleu([ref_text.split()], hyp_text.split(), \n",
    "                         weights=(0.5, 0.5, 0, 0), \n",
    "                         smoothing_function=smoothing_function)\n",
    "\n",
    "ngram_ranges_word = [(1,2)]\n",
    "ngram_ranges_char = [(2,4)]\n",
    "max_df_values = [0.8]\n",
    "min_df_values = [1]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for ngram_range, max_df, min_df in [(a, b, c) for a in ngram_ranges_word for b in max_df_values for c in min_df_values]:\n",
    "    vectorizer, tfidf_train, tfidf_test = compute_tfidf(X_train, X_test, ngram_range, ngram_ranges_char[0], max_df, min_df)\n",
    "    \n",
    "    X_test = X_test.copy()\n",
    "    X_test['retrieved_response'] = find_best_responses(tfidf_train, tfidf_test, X_train)\n",
    "\n",
    "    X_test['bleu_score'] = X_test.apply(lambda row: compute_bleu(row['model_response'], row['retrieved_response']), axis=1)\n",
    "    avg_bleu = X_test['bleu_score'].mean()\n",
    "    print(f'BLEU (w_gram: {ngram_range}, max_df={max_df}, min_df={min_df}): {avg_bleu}')\n",
    "\n",
    "    if avg_bleu > best_score:\n",
    "        best_score = avg_bleu\n",
    "        best_params = (ngram_range, max_df, min_df)\n",
    "\n",
    "print(f'Best Params: {best_params} with BLEU Score: {best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3aad9f-f6f1-4760-84f8-a780a21f5e91",
   "metadata": {},
   "source": [
    "## Track II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04b2469d-b932-430e-9e87-72e1203ba485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0892002592467896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load Data\n",
    "def load_data():\n",
    "    dev_responses = pd.read_csv('dev_responses.csv')\n",
    "    train_responses = pd.read_csv('train_responses.csv')\n",
    "    return pd.concat([dev_responses, train_responses])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    # text = re.sub(r'\\b\\d+\\b', '', text)  # Remove isolated numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "original_data = load_data()\n",
    "original_data['preprocess_prompt'] = original_data['user_prompt'].apply(preprocess_text)\n",
    "\n",
    "X_train, X_test = train_test_split(original_data, test_size=0.2, random_state=21)\n",
    "\n",
    "X_train['preprocess_prompt'] = X_train['preprocess_prompt'].astype(str)\n",
    "X_train['model_response'] = X_train['model_response'].astype(str)\n",
    "X_test['model_response'] = X_test['model_response'].astype(str)\n",
    "\n",
    "text_model = api.load(\"word2vec-google-news-300\")\n",
    "# text_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
    "\n",
    "# Get embedding for a single prompt\n",
    "def get_embedding(prompt, model):\n",
    "    words = prompt.split()\n",
    "    valid_vectors = [model[tok] for tok in words if tok in model.key_to_index]\n",
    "    # return np.median(valid_vectors, axis=0) if valid_vectors else np.zeros(model.vector_size)\n",
    "    return np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "X_train_embeddings = np.vstack([get_embedding(p, text_model) for p in X_train['preprocess_prompt']])\n",
    "X_test_embeddings = np.vstack([get_embedding(p, text_model) for p in X_test['preprocess_prompt']])\n",
    "\n",
    "similarities = cosine_similarity(X_test_embeddings, X_train_embeddings)\n",
    "# similarities = -euclidean_distances(X_test_embeddings, X_train_embeddings)\n",
    "\n",
    "top_indices = np.argmax(similarities, axis=1)\n",
    "retrieved_responses = X_train.iloc[top_indices]['model_response'].values\n",
    "\n",
    "X_test['retrieved_response']=retrieved_responses\n",
    "\n",
    "smoothing_function = SmoothingFunction().method3\n",
    "\n",
    "def compute_bleu(ref_text, hyp_text):\n",
    "    return sentence_bleu([ref_text.split()], hyp_text.split(), \n",
    "                         weights=(0.5, 0.5, 0, 0), \n",
    "                         smoothing_function=smoothing_function)\n",
    "\n",
    "X_test['bleu_score'] = X_test.apply(lambda row: compute_bleu(row['model_response'], row['retrieved_response']), axis=1)\n",
    "\n",
    "average_bleu = X_test['bleu_score'].mean()\n",
    "print(f'Average BLEU Score: {average_bleu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f05a6f-232c-44c6-8b69-f148ab0199fd",
   "metadata": {},
   "source": [
    "## Track III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b8f1bd3-96d3-4906-b9a9-01338fc5422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU for alpha=0.1 is: 0.09765004822105479\n",
      "Average BLEU for alpha=0.2 is: 0.10226067224053291\n",
      "Average BLEU for alpha=0.3 is: 0.1062946072944321\n",
      "Average BLEU for alpha=0.4 is: 0.1083045350373584\n",
      "Average BLEU for alpha=0.5 is: 0.10973773194184341\n",
      "Average BLEU for alpha=0.6 is: 0.11079854422692353\n",
      "Average BLEU for alpha=0.7 is: 0.1101675598143283\n",
      "Average BLEU for alpha=0.8 is: 0.11022807233311652\n",
      "Average BLEU for alpha=0.9 is: 0.11005694972271751\n",
      "Average BLEU for alpha=1.0 is: 0.10856402947082004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Data\n",
    "def load_data():\n",
    "    dev_responses = pd.read_csv('dev_responses.csv')\n",
    "    train_responses = pd.read_csv('train_responses.csv')\n",
    "    return pd.concat([dev_responses, train_responses])\n",
    "\n",
    "# Preprocess Text: Lowercase, strip, remove punctuation and digits\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "    return text\n",
    "\n",
    "# Load data and preprocess prompts\n",
    "original_data = load_data()\n",
    "original_data['preprocess_prompt'] = original_data['user_prompt'].apply(preprocess_text)\n",
    "\n",
    "# Split into train and test sets (80% train, 20% test)\n",
    "X_train, X_test = train_test_split(original_data, test_size=0.2, random_state=100)\n",
    "\n",
    "# Ensure necessary columns are in string format\n",
    "X_train['preprocess_prompt'] = X_train['preprocess_prompt'].astype(str)\n",
    "X_train['model_response'] = X_train['model_response'].astype(str)\n",
    "X_test['model_response'] = X_test['model_response'].astype(str)\n",
    "\n",
    "# Load BERT-based Sentence Transformer Model\n",
    "# bert_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "bert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Compute sentence embeddings for all training prompts\n",
    "X_train_embeddings = bert_model.encode(X_train['preprocess_prompt'].tolist(), convert_to_tensor=True)\n",
    "X_test_embeddings = bert_model.encode(X_test['preprocess_prompt'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities between test and train embeddings (semantic similarity)\n",
    "similarities = cosine_similarity(X_test_embeddings.cpu().numpy(), X_train_embeddings.cpu().numpy())\n",
    "\n",
    "# Compute lexical similarities using TF-IDF\n",
    "ngram_range_w = (1, 2)\n",
    "ngram_range_c = (2, 4)\n",
    "max_df = 0.9\n",
    "min_df = 1\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=ngram_range_w, max_df=max_df, min_df=min_df, analyzer='word', sublinear_tf=True)\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=ngram_range_c, max_df=max_df, min_df=min_df, analyzer='char', sublinear_tf=True)\n",
    "tfidf = FeatureUnion([(\"word_tfidf\", word_vectorizer), (\"char_tfidf\", char_vectorizer)])\n",
    "\n",
    "# tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['preprocess_prompt'])\n",
    "X_test_tfidf = tfidf.transform(X_test['preprocess_prompt'])\n",
    "lexical_similarities = cosine_similarity(X_test_tfidf, X_train_tfidf)\n",
    "\n",
    "# Combine lexical and semantic similarities\n",
    "for i in range(1,11,1):#[0.8]:#\n",
    "    alpha = i/10  # Weight for semantic similarity\n",
    "    \n",
    "    combined_similarities = alpha * similarities + (1 - alpha) * lexical_similarities\n",
    "    \n",
    "    # Find the index of the most similar train response for each test response\n",
    "    top_indices = np.argmax(combined_similarities, axis=1)\n",
    "    \n",
    "    # Retrieve the corresponding responses from the train set\n",
    "    X_test['retrieved_response'] = X_train.iloc[top_indices]['model_response'].values\n",
    "    \n",
    "    # BLEU Score Calculation\n",
    "    smoothing_function = SmoothingFunction().method3\n",
    "    \n",
    "    def compute_bleu(ref_text, hyp_text):\n",
    "        return sentence_bleu([ref_text.split()], hyp_text.split(), \n",
    "                             weights=(0.5, 0.5, 0, 0), \n",
    "                             smoothing_function=smoothing_function)\n",
    "    \n",
    "    X_test['bleu_score'] = X_test.apply(lambda row: compute_bleu(row['model_response'], row['retrieved_response']), axis=1)\n",
    "    \n",
    "    # Final average BLEU score\n",
    "    average_bleu = X_test['bleu_score'].mean()\n",
    "    print(f'Average BLEU for alpha={alpha} is: {average_bleu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febb21c-cfbf-4c2d-a27d-5e81dd1ef3a5",
   "metadata": {},
   "source": [
    "## TRIAL II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1e22362-c7e6-4b51-8fc2-52ee117eecc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU for alpha=0.1 is: 0.09763591275616758\n",
      "Average BLEU for alpha=0.2 is: 0.10228284042124845\n",
      "Average BLEU for alpha=0.3 is: 0.10635380183359866\n",
      "Average BLEU for alpha=0.4 is: 0.10819860105729108\n",
      "Average BLEU for alpha=0.5 is: 0.10971284244252758\n",
      "Average BLEU for alpha=0.6 is: 0.110796625131974\n",
      "Average BLEU for alpha=0.7 is: 0.1101675598143283\n",
      "Average BLEU for alpha=0.8 is: 0.11022807233311652\n",
      "Average BLEU for alpha=0.9 is: 0.11005694972271751\n",
      "Average BLEU for alpha=1.0 is: 0.10856402947082004\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_data():\n",
    "    dev_responses = pd.read_csv('dev_responses.csv')\n",
    "    train_responses = pd.read_csv('train_responses.csv')\n",
    "    return pd.concat([dev_responses, train_responses])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "original_data = load_data()\n",
    "original_data['preprocess_prompt'] = original_data['user_prompt'].apply(preprocess_text)\n",
    "\n",
    "X_train, X_test = train_test_split(original_data, test_size=0.2, random_state=100)\n",
    "\n",
    "X_train['preprocess_prompt'] = X_train['preprocess_prompt'].astype(str)\n",
    "X_train['model_response'] = X_train['model_response'].astype(str)\n",
    "X_test['model_response'] = X_test['model_response'].astype(str)\n",
    "\n",
    "# bert_model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "bert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "X_train_embeddings = bert_model.encode(X_train['preprocess_prompt'].tolist(), convert_to_tensor=True)\n",
    "X_test_embeddings = bert_model.encode(X_test['preprocess_prompt'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "similarities = cosine_similarity(X_test_embeddings.cpu().numpy(), X_train_embeddings.cpu().numpy())\n",
    "\n",
    "ngram_range_w = (1, 2)\n",
    "ngram_range_c = (2, 4)\n",
    "max_df = 0.8\n",
    "min_df = 1\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=ngram_range_w, max_df=max_df, min_df=min_df, analyzer='word', sublinear_tf=True)\n",
    "char_vectorizer = TfidfVectorizer(ngram_range=ngram_range_c, max_df=max_df, min_df=min_df, analyzer='char', sublinear_tf=True)\n",
    "tfidf = FeatureUnion([(\"word_tfidf\", word_vectorizer), (\"char_tfidf\", char_vectorizer)])\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['preprocess_prompt'])\n",
    "X_test_tfidf = tfidf.transform(X_test['preprocess_prompt'])\n",
    "lexical_similarities = cosine_similarity(X_test_tfidf, X_train_tfidf)\n",
    "\n",
    "for i in range(1,11,1):\n",
    "    alpha = i/10\n",
    "    \n",
    "    combined_similarities = alpha * similarities + (1 - alpha) * lexical_similarities\n",
    "    \n",
    "    top_indices = np.argmax(combined_similarities, axis=1)\n",
    "    \n",
    "    X_test['retrieved_response'] = X_train.iloc[top_indices]['model_response'].values\n",
    "    \n",
    "    smoothing_function = SmoothingFunction().method3\n",
    "    def compute_bleu(ref_text, hyp_text):\n",
    "        return sentence_bleu([ref_text.split()], hyp_text.split(), \n",
    "                             weights=(0.5, 0.5, 0, 0), \n",
    "                             smoothing_function=smoothing_function)\n",
    "    \n",
    "    X_test['bleu_score'] = X_test.apply(lambda row: compute_bleu(row['model_response'], row['retrieved_response']), axis=1)\n",
    "    \n",
    "    average_bleu = X_test['bleu_score'].mean()\n",
    "    print(f'Average BLEU for alpha={alpha} is: {average_bleu}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d45dad99-b983-4d8c-bd7c-0101030b2c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from rank_bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ca8c297-5cbb-4f72-aefa-10f6456494d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0\n",
      "Uninstalling torch-2.6.0:\n",
      "  Successfully uninstalled torch-2.6.0\n",
      "Found existing installation: transformers 4.50.0\n",
      "Uninstalling transformers-4.50.0:\n",
      "  Successfully uninstalled transformers-4.50.0\n",
      "Found existing installation: sentence-transformers 3.4.1\n",
      "Uninstalling sentence-transformers-3.4.1:\n",
      "  Successfully uninstalled sentence-transformers-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Lenovo\\anaconda3\\envs\\myenv\\Lib\\site-packages\\~orch'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch transformers sentence-transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df5abda5-eca4-41b1-81e5-56738fe7d75e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No matching packages\n"
     ]
    }
   ],
   "source": [
    "# !pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e512e9ca-636b-4fbb-ba2b-c9d5660c857d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.6.0%2Bcpu-cp312-cp312-win_amd64.whl (206.5 MB)\n",
      "   ---------------------------------------- 0.0/206.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/206.5 MB 4.8 MB/s eta 0:00:44\n",
      "    --------------------------------------- 2.6/206.5 MB 6.9 MB/s eta 0:00:30\n",
      "    --------------------------------------- 3.7/206.5 MB 6.2 MB/s eta 0:00:33\n",
      "    --------------------------------------- 5.0/206.5 MB 6.4 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 6.6/206.5 MB 6.8 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 8.1/206.5 MB 6.9 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 9.2/206.5 MB 7.0 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 11.3/206.5 MB 6.8 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 12.6/206.5 MB 6.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 14.4/206.5 MB 7.0 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 16.3/206.5 MB 7.2 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 17.6/206.5 MB 7.2 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 18.6/206.5 MB 7.0 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 20.4/206.5 MB 7.0 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 22.5/206.5 MB 7.2 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 24.4/206.5 MB 7.3 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 26.2/206.5 MB 7.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 27.8/206.5 MB 7.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 29.4/206.5 MB 7.4 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 31.2/206.5 MB 7.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 32.8/206.5 MB 7.5 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 34.6/206.5 MB 7.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 36.2/206.5 MB 7.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 37.7/206.5 MB 7.6 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 39.6/206.5 MB 7.6 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 41.2/206.5 MB 7.6 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 42.2/206.5 MB 7.7 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 44.3/206.5 MB 7.6 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 46.4/206.5 MB 7.7 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 47.7/206.5 MB 7.6 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 48.8/206.5 MB 7.5 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 50.9/206.5 MB 7.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 52.2/206.5 MB 7.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 53.7/206.5 MB 7.6 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 55.3/206.5 MB 7.6 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 56.6/206.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 58.2/206.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 60.0/206.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 61.1/206.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 63.2/206.5 MB 7.5 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 65.3/206.5 MB 7.6 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 66.8/206.5 MB 7.6 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 68.2/206.5 MB 7.5 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 69.2/206.5 MB 7.5 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 71.0/206.5 MB 7.5 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 71.0/206.5 MB 7.5 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 72.9/206.5 MB 7.4 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 74.4/206.5 MB 7.4 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 76.3/206.5 MB 7.4 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 77.9/206.5 MB 7.4 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 80.2/206.5 MB 7.5 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 82.1/206.5 MB 7.5 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 84.1/206.5 MB 7.5 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 85.7/206.5 MB 7.5 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 87.3/206.5 MB 7.5 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 87.8/206.5 MB 7.5 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 88.6/206.5 MB 7.4 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 88.6/206.5 MB 7.4 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 89.4/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 89.7/206.5 MB 7.2 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 93.1/206.5 MB 6.3 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 93.3/206.5 MB 6.3 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 94.1/206.5 MB 6.2 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 94.9/206.5 MB 6.2 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 95.2/206.5 MB 6.1 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 96.5/206.5 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 97.3/206.5 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 97.8/206.5 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 99.9/206.5 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 99.9/206.5 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.7/206.5 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 101.7/206.5 MB 6.0 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 102.2/206.5 MB 5.9 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 103.0/206.5 MB 5.9 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 104.9/206.5 MB 5.9 MB/s eta 0:00:18\n",
      "   -------------------- ------------------- 106.2/206.5 MB 5.9 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 107.5/206.5 MB 5.9 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 109.1/206.5 MB 6.0 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 110.9/206.5 MB 6.0 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 112.5/206.5 MB 6.0 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 114.3/206.5 MB 6.0 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 116.4/206.5 MB 6.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 117.7/206.5 MB 6.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 120.1/206.5 MB 6.1 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 121.4/206.5 MB 6.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 121.4/206.5 MB 6.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 122.4/206.5 MB 6.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 122.9/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 124.0/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 124.3/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 125.6/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 127.1/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------ --------------- 128.7/206.5 MB 6.0 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 129.5/206.5 MB 6.0 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 130.8/206.5 MB 6.0 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 132.1/206.5 MB 6.0 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 133.7/206.5 MB 6.0 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 134.5/206.5 MB 6.0 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 135.0/206.5 MB 5.9 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 136.6/206.5 MB 6.0 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 137.1/206.5 MB 5.9 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 137.9/206.5 MB 5.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 139.5/206.5 MB 5.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 141.0/206.5 MB 5.9 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 141.8/206.5 MB 5.9 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 143.7/206.5 MB 5.9 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 144.4/206.5 MB 5.9 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 145.8/206.5 MB 5.9 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 147.3/206.5 MB 5.9 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 148.6/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 149.4/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 150.2/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 151.5/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 152.6/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 152.8/206.5 MB 5.9 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 153.6/206.5 MB 5.8 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 154.7/206.5 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 156.0/206.5 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 157.5/206.5 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 158.3/206.5 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 158.3/206.5 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 160.4/206.5 MB 5.8 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 161.7/206.5 MB 5.8 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 163.1/206.5 MB 5.8 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 164.6/206.5 MB 5.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 165.9/206.5 MB 5.8 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 167.0/206.5 MB 5.8 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 168.6/206.5 MB 5.8 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 169.9/206.5 MB 5.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 171.2/206.5 MB 5.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 173.0/206.5 MB 5.8 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 174.3/206.5 MB 5.9 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 175.4/206.5 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 176.2/206.5 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 176.4/206.5 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 177.5/206.5 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 178.8/206.5 MB 5.8 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 180.1/206.5 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 180.9/206.5 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 181.1/206.5 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 182.5/206.5 MB 5.7 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 183.8/206.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 185.3/206.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 186.9/206.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 188.0/206.5 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 189.8/206.5 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 191.4/206.5 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 192.2/206.5 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 192.7/206.5 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 192.9/206.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 192.9/206.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 193.2/206.5 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 194.0/206.5 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 194.2/206.5 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 194.2/206.5 MB 5.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 196.1/206.5 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 197.1/206.5 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 198.2/206.5 MB 5.3 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 199.2/206.5 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 200.0/206.5 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 201.3/206.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.4/206.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.2/206.5 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  204.5/206.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  205.8/206.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  206.3/206.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  206.3/206.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  206.3/206.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 206.5/206.5 MB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bac0d072-5a41-45d4-bbf1-b859265e9e0b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.2 MB 3.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/10.2 MB 4.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/10.2 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/10.2 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/10.2 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.0/10.2 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.2 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "Successfully installed sentence-transformers-3.4.1 transformers-4.50.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers sentence-transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
